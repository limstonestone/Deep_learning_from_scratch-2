{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap02_Distributional_representation_of_natural_language_and_word.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMC9BsqtqEFU3ES+ZWl3BHP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0hzFyRkVHfA",
        "outputId": "284a8d41-157d-46d3-a732-ed9f3d4c37ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **자연어 처리란**"
      ],
      "metadata": {
        "id": "3gNzG1N5V1u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**자연어(natural language)** : 한국어와 영어 등 우리가 평소에 쓰는 말\n",
        "\n",
        "**자연어처리(natural language preprocessing : NLP)** : 우리의 말을 컴퓨터에게 이해시키기 위한 기술(분야)\n",
        "\n",
        "자연어는 살아 있는 언어이며 컴퓨터 언어에 비해서 '부드러움'이 있다.\n",
        "\n",
        "때문에 '딱딱한' 컴퓨터언어에 자연어를 이해시키기란 어려운 일이다."
      ],
      "metadata": {
        "id": "wABsWqMPWQwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **단어의 의미**"
      ],
      "metadata": {
        "id": "-uCZHlnEXBiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어는 말하자면 의미의 최소 단위이다.\n",
        "\n",
        "컴퓨터에게 '단어의 이미'를 이해시키는 방법에는 다음 세가지 기법이 있다.\n",
        "\n",
        "* 시소러스를 활용한 기법(이번 장)\n",
        "* 통계 기반 기법(이번 장)\n",
        "* 추론 기반 기법(word2vec) (다음장)\n",
        "\n",
        "가장 먼저 사람의 손으로 만든 시소러스(thesaurus, 유의어사전)를 이용하는 방법을 간단히 살펴 보자."
      ],
      "metadata": {
        "id": "FuNO4ZznXD2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **시소러스**"
      ],
      "metadata": {
        "id": "UuFswMMOXi0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'단어의 의미'를 나타내는 방법으로는 먼저 사람이 직접 단어의 의미를 정의하는 방식을 생각할 수 있다.\n",
        "\n",
        "시소러스란 (기본적으로는) 유의어 사전으로, '뜻이 같은 단어(동의어)'나 '뜻이 비슷한 단어(유의어)'가 한 그룹으로 분류되어 있다.\n",
        "\n",
        "ex) car = auto / automobile / machine / motocar\n",
        "\n",
        "또한 시소러스에는 단어 사이의 '상위와 하위' 혹은 '전체와 부분'등 더 세세한 관계까지 정의 하기도 한다.\n",
        "\n",
        "ex)   \n",
        "object  \n",
        "|  \n",
        "motor vehicle  \n",
        "|  \n",
        "car / truck\n",
        "\n",
        "이처럼 단어들의 관계로 단어 사이의 연결을 정의하는 '단어 네트워크'를 이용하여 컴퓨터에게 단어사이의 관계를 가르칠 수 있다. 그러면 간접적으로라도 컴퓨터에게 단어의 의미를 이해시켰다고 주장할 수 있을 것이다."
      ],
      "metadata": {
        "id": "cI5vchkIXkwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **WordNet**"
      ],
      "metadata": {
        "id": "lzz8h5NvYwJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "자연어 처리 분야에서 가장 유명한 시소러스는 **WordNet**이다.\n",
        "\n",
        "WordNet을 사용하면 유의어를 얻거나 '단어 네트워크'를 이용할 수 있다. 또한 단어 네트워크를 사용해 단어 사이의 유사도 또한 구할 수 있다."
      ],
      "metadata": {
        "id": "hOzZYNJDYzeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **시소러스의 문제점**"
      ],
      "metadata": {
        "id": "E7TkZvMUZF_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사람이 수작업으로 레이블링하는 방식에는 크나큰 결점이 존재할 수 있다. 다음은 시소러스 방식의 대표적인 문제점들이다.\n",
        "\n",
        "* **시대 변화에 대응하기 어렵다.**\n",
        "\n",
        "    때때로 새로운 단어가 생겨나고, 구닥다리 옛말은 언젠가 잊혀진다. 또한 시대에 따라 언어의 의미가 변하기도 한다. 이런 단어의 변화에 대응하려면 시소러스를 사람이 수작업으로 끊임없이 갱신해야 한다.\n",
        "\n",
        "* **사람을 쓰는 비용이 크다**\n",
        "\n",
        "    시소러스를 만드는 데는 엄청난 인적 비용이 발생한다.\n",
        "\n",
        "* **단어의 미묘한 차이를 표현할 수 없다.**\n",
        "\n",
        "    실제로 비슷한 단어들이라도 미묘한 차이가 있는 법이다. 시소러스에서는 이러한 미묘한 차이를 표현할 수 없다(이 역시 수작업으로 표현하려 한다면 상당히 곤란한 일이 될 수 있다).  \n",
        "\n",
        "이렇듯 시소러스를 사용하는 기법에는 문제가 많다.\n",
        "\n",
        "'통계 기반 기법'과 신경망을 사용한 '추론 기반 기법'을 이용하면 대량의 텍스트 데이터로부터 '단어의 의미'를 자동으로 추출한다. 이 덕분에 사람이 순수 단어를 연결짓는 중노동에서 해방된다."
      ],
      "metadata": {
        "id": "Qg_TbhnxZHqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **통계 기반 기법**"
      ],
      "metadata": {
        "id": "YwUZl877aHum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "통계 기반 기법에서는 자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 텍스트 데이터인 **말뭉치(corpus)**를 이용한다.\n",
        "\n",
        "말뭉치란 사람이 쓴 글이므로, 자연어에 대한 사람의 '지식'이 충분히 담겨있다고 볼 수 있다. 통계 기반 기법의 목표는 이처럼 사람의 지식으로 가득한 말뭉치에서 자동으로, 그리고 효율적으로 그 핵심을 추출하는 것이다."
      ],
      "metadata": {
        "id": "wneObxP5aLI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **파이썬으로 말뭉치 전처리하기**"
      ],
      "metadata": {
        "id": "Y0ksy3J-b62m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text를 단어 단위로 분할\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "\n",
        "text = text.lower()\n",
        "text = text.replace('.', ' .')      # 정규표현식을 이용하면 조금 더 간단하게 표현이 가능하긴 하지만 다루지 않겠다.\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aDDGrDbncA0T",
        "outputId": "4bfaddc8-036b-4e18-df4f-21ab9672b8bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you say goodbye and i say hello .'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split(' ')\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHfn9Ge9cP8M",
        "outputId": "4bf2014e-4ad4-4a5b-84c9-5603a9ec7623"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어를 조작하기 쉽게 ID 부여\n",
        "\n",
        "word_to_id = {}\n",
        "id_to_word = {}\n",
        "\n",
        "for word in words:\n",
        "    if word not in word_to_id:\n",
        "        new_id = len(word_to_id)\n",
        "        word_to_id[word] = new_id\n",
        "        id_to_word[new_id] = word"
      ],
      "metadata": {
        "id": "lGkh8dihceMy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ws7XLTAczc_",
        "outputId": "3afcba4d-f80a-4ca7-ebbf-5b4d32116b0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRbI4cD1c9FU",
        "outputId": "57d54cc3-ef92-4be0-d42f-28b23e2fb6b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 6, 'and': 3, 'goodbye': 2, 'hello': 5, 'i': 4, 'say': 1, 'you': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 딕셔너리를 사용하면 단어와 단어ID 검색가능\n",
        "\n",
        "id_to_word[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aW1kj52Kc-W0",
        "outputId": "82a53981-5344-4106-8407-1c3f20f96e3b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'say'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_id['hello']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOzOhKTodAN8",
        "outputId": "21527910-fb5c-4754-d291-752875528de2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# '단어 목록'을 '단어 ID 목록'으로 변경\n",
        "\n",
        "import numpy as np\n",
        "corpus = [word_to_id[w] for w in words]\n",
        "corpus = np.array(corpus)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTKmF-f9dF5B",
        "outputId": "19347c3e-14a5-42d4-c8e5-ecbd441389ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 1, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수로 구현하기\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word"
      ],
      "metadata": {
        "id": "DvrTBUMadZfg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수를 사용하여 말뭉치 전처리\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)"
      ],
      "metadata": {
        "id": "1CEUSKCAdrJq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **단어의 분산 표현**"
      ],
      "metadata": {
        "id": "zladz56PdzlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'색'을 벡터로 표현하듯 '단어'도 벡터로 표현할 수 있을까?\n",
        "\n",
        "우리가 원하는 것은 '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현이다.\n",
        "\n",
        "이를 자연어 처리 분야에서는 단어의 **분산표현(distributional representation)**이라고 한다."
      ],
      "metadata": {
        "id": "iWP4Hj-Fd55G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **분포 가설**"
      ],
      "metadata": {
        "id": "DL5AhGXWebFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**분포 가설(distributional hypothesis)** : '단어의 의미는 주변 단어에 의해 형성된다'라는 가설\n",
        "\n",
        "분포 가설이 말하고자 하는 바는 단어 자체에는 의미가 없고, 그 단어가 사용된 '맥락(context)'이 의미를 형성한다는 것이다.\n",
        "\n",
        "'맥락'이란 특정 단어를 중심에 둔 그 주변 단어를 말한다. 그리고 맥락의 크기(주변 단어를 몇 개나 포함할지)를 '윈도우 크기(window size)'라고 한다. 윈도우 크기가 1이면 좌우 한 단어씩, 윈도우 크기가 2이면 좌우 두 단어씩 맥락에 포함된다. (굳이 좌우 동수가 될 필욘 없다.)"
      ],
      "metadata": {
        "id": "YLSnOQDzfP08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **동시발생 행렬**"
      ],
      "metadata": {
        "id": "0rBbsd8xfzAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "분포 가설에 기초해 단어를 벡터로 나타내는 방법을 생각해 보면, 주변 단어를 '세어보는'방법이 떠오른다. 즉 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇번이나 등장하는지를 세어 집계하는 방법이다. 이를 '통계 기반(statistical based)' 기법이라고 하겠다.\n",
        "\n",
        "이제 통계 기반 기법을 살펴보자. 집계한 결과를 행렬로 나타낼 것이다."
      ],
      "metadata": {
        "id": "_br61gQTf1Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_path = '/content/drive/MyDrive/밑바닥부터시작하는딥러닝2/deep-learning-from-scratch-2-master'\n",
        "\n",
        "import sys\n",
        "sys.path.append(my_path)\n",
        "from common.util import preprocess\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "print(id_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4DgQ3LGgVrN",
        "outputId": "01f7b101-bcb1-4b92-9a77-cde9300620f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 1 5 6]\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "    '''동시발생 행렬 생성\n",
        "    :param corpus: 말뭉치(단어 ID 목록)\n",
        "    :param vocab_size: 어휘 수\n",
        "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
        "    :return: 동시발생 행렬\n",
        "    '''\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(1, window_size + 1):\n",
        "            left_idx = idx - i\n",
        "            right_idx = idx + i\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix"
      ],
      "metadata": {
        "id": "xWHzXYxJgnWO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_co_matrix(corpus, 7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW9IFQANg6YW",
        "outputId": "8f8657cd-5dcb-471c-989d-e31d2d56fcf1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 1, 0, 0],\n",
              "       [0, 1, 0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 1, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 **동시발생 행렬(co-occurrence matrix)**라고 한다.\n",
        "\n",
        "예를 들자면, 첫 행은 'you'이고, 두번째 열은 'say'이므로, 첫 번째 행을 살펴보면 'you'의 근처에는 'say'가 한 번 카운트 되고 나머지 단어는 없음을 알 수 있다."
      ],
      "metadata": {
        "id": "hDq8Gyt2g909"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **벡터 간 유사도**"
      ],
      "metadata": {
        "id": "O4Q3ki7uhfZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "벡터 사이의 유사도를 측정하는 방법은 다양하다. 대표적으로는 벡터의 내적이나 유클리드 거리 등을 꼽을 수 있는데, 단어 벡터의 유사도를 나타낼 때는 **코사인 유사도(cosine similarity)**를 자주 이용한다.\n",
        "\n",
        "$$similarity=cos(\\theta)=\\frac{x⋅y}{||x||\\ ||y||}=\\frac{\\sum_{i=1}^{n}{x_{i}×y_{i}}}{\\sqrt{\\sum_{i=1}^{n}(x_{i})^2}×\\sqrt{\\sum_{i=1}^{n}(y_{i})^2}}$$  \n",
        "\n",
        "이 식의 핵심은 벡터를 정규화하고 내적을 구하는 것이다. 직관적으로 풀어보자면 '두 벡터가 가리키는 방향이 얼마나 비슷한가'이다. 두 벡터의 방향이 완전히 같다면 코사인 유사도가 1이 되며, 완전히 반대라면 -1이 된다.\n",
        "\n",
        "이제, 코사인 유사도를 파이썬 함수로 구현해보자."
      ],
      "metadata": {
        "id": "cMwiQRzThkG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(x, y, eps=1e-8):       # 엡실론(epsilon)의 도입으로 분모를 0으로 나누지 않게 된다.\n",
        "    nx = x / np.sqrt(np.sum(x**2) + eps)  # x의 정규화\n",
        "    ny = y / np.sqrt(np.sum(y**2) + eps)  # y의 정규화\n",
        "    return np.dot(nx, ny)"
      ],
      "metadata": {
        "id": "TU55a2nWibxt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 통해 \"you\"와 \"i(=I)\"의 유사도를 구하는 코드를 확인해보자."
      ],
      "metadata": {
        "id": "-EuzhafXikJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from common.util import preprocess, create_co_matrix, cos_similarity\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "c0 = C[word_to_id['you']]  # \"you\"의 단어 벡터\n",
        "c1 = C[word_to_id['i']]    # \"i\"의 단어 벡터\n",
        "print(cos_similarity(c0, c1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1XuRqgSi7bl",
        "outputId": "65141f23-0470-48a6-bca0-6a72b5cf12cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7071067691154799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "비교적 유사성이 크다고 말할 수 있다."
      ],
      "metadata": {
        "id": "bsAr1s9rjOmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **유사 단어의 랭킹 표시**"
      ],
      "metadata": {
        "id": "01awi-UzjT3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 구현해보자."
      ],
      "metadata": {
        "id": "nRMwiRe6jV0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    '''유사 단어 검색\n",
        "    :param query: 쿼리(텍스트)\n",
        "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
        "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
        "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
        "    :param top: 상위 몇 개까지 출력할 지 지정\n",
        "    '''\n",
        "    if query not in word_to_id:\n",
        "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    # 코사인 유사도 계산\n",
        "    vocab_size = len(id_to_word)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return"
      ],
      "metadata": {
        "id": "ispW9cndjcDS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "most_similar('you', word_to_id, id_to_word, C, top=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBmUDHRmlIuU",
        "outputId": "b2f3c5bf-67fb-4440-cec0-22927cdff219"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[query] you\n",
            " goodbye: 0.7071067691154799\n",
            " i: 0.7071067691154799\n",
            " hello: 0.7071067691154799\n",
            " say: 0.0\n",
            " and: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hello와 goodbye가 코사인 유사도가 높은것은 직관과는 거리가 멀다. 지금의 경우 말뭉치의 크기가 너무 작기 때문에 이러한 문제가 발생하는 것이다. 이것으로 통계 기반 기법의 '기본'을 마친다."
      ],
      "metadata": {
        "id": "bd6IVe76lj1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **통계 기반 기법 개선하기**"
      ],
      "metadata": {
        "id": "K5DrdIMfl1nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사실 동시발생 행렬에는 개선할 점이 있다. 이번에는 개선 작업을 수행한 후 더 실용적인 말뭉치를 사용하여 '진짜'단어의 분산 표현을 획득한다."
      ],
      "metadata": {
        "id": "w-6TYQGBl71k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **상호정보량**"
      ],
      "metadata": {
        "id": "Bu030jLkmFCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사실 동시발생 행렬의 원소인 '발생'횟수라는 것은 사실 그리 좋은 특징이 아니다. 고빈도 단어로 예를 들어보자.\n",
        "\n",
        "\"the\"와 \"car\"의 동시발생을 생각해보자. 분명히 \"... the car ...\"라는 문구가 자주 보일 것이다. 따라서 두 단어의 동시발생 횟수는 아주 많다. 한편, \"car\"와 \"drive\"는 연관이 아주 깊다. 하지만 단순히 등장 횟수만 보자면 \"the\"가 훨씬 많다. 결국 \"the\"가 고빈도 단어라서 \"car\"와 강한 관련성을 갖는다고 결론 짓게 되는 것이다.\n",
        "\n",
        "이를 해결하기 위해 **점별 상호정보량(Pointwise Mutual Information, PMI)**라는 척도를 사용한다.\n",
        "\n",
        "$$PMI(x,y) = {log_2}\\frac{P(x,y)}{P(x)P(y)}$$\n",
        "\n",
        "$P(x)$는 $x$가 일어날 확률, $P(x,y)$는 x와 y가 동시에 일어날 확률을 뜻한다. 이 PMI 값이 높을수록 관련성이 높다는 의미이다.\n",
        "\n",
        "동시발생 행렬 $C$을 이용해서 식을 다시 써보자.\n",
        "\n",
        "$$PMI(x,y) = {log_2}\\frac{P(x,y)}{P(x)P(y)}=log_2\\frac{\\frac{C(x,y)}{N}}{\\frac{C(x)}{N}\\frac{C(y)}{N}}=log_2\\frac{C(x,y)N}{C(x)C(y)}$$\n",
        "\n",
        "따라서 동시발생 핼렬로부터 PMI를 구할 수 있다.\n",
        "\n",
        "앞서 든 예시를 PMI 관점에서 바라보자.\n",
        "\n",
        "$$PMI(\"the\",\"car\")=log_2\\frac{10*10000}{1000*20}\\approx2.32$$\n",
        "\n",
        "$$PMI(\"car\",\"drive\")=log_2\\frac{5*10000}{20*10}\\approx7.97$$\n",
        "\n",
        "이 결과에서 알 수 있듯이 PMI를 이용하면 \"car\"는 \"the\"보다 \"drive\"와의 관련성이 강해진다. 이는 우리가 원하던 결과이다. 이 예에서는 \"the\"가 자주 출현했으므로 PMI점수가 낮아진 것이다.\n",
        "\n",
        "하지만 PMI척도에도 한가지 문제가 있다. 동시발생 횟수가 0이면 $log_20 = -\\infty$가 된다는 점이다. 이 문제를 피하기 위해 실제 구현 할때는 **양의 상호정보량(Positive PMI, PPMI)** 을 사용한다.\n",
        "\n",
        "$$PPMI(x, y) = max(0, PMI(x,y))$$\n",
        "\n",
        "이 식에 따라 PMI가 음수일 때는 0으로 취급한다.\n",
        "\n",
        "이제, 동시발생 행렬을 PPMI 행렬로 변환하는 함수를 구현해보자."
      ],
      "metadata": {
        "id": "BZ10CmQ9mPFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ppmi(C, verbose=False, eps = 1e-8):\n",
        "    '''PPMI(점별 상호정보량) 생성\n",
        "    :param C: 동시발생 행렬\n",
        "    :param verbose: 진행 상황을 출력할지 여부\n",
        "    :return:\n",
        "    '''\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100 + 1) == 0:\n",
        "                    print('%.1f%% 완료' % (100*cnt/total))\n",
        "    return M"
      ],
      "metadata": {
        "id": "Z9WpBHTlqn6s"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "동시발생 행렬을 PPMI 행렬로 변환해보자."
      ],
      "metadata": {
        "id": "yXKt6a91qtUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "np.set_printoptions(precision=3) # 유효 자릿수를 세 자리로 표시\n",
        "print('동시발생 행렬')\n",
        "print(C)\n",
        "print('-'*50)\n",
        "print('PPMI')\n",
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGrlhuheq7M6",
        "outputId": "73c0d0b3-0fb7-47b8-9b2b-cb8a876a1ba1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "동시발생 행렬\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 1 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]]\n",
            "--------------------------------------------------\n",
            "PPMI\n",
            "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.807 0.    0.    0.    0.    2.807]\n",
            " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "동시발생 행렬보다 더 좋은 척도로 이뤄진 행렬을 손에 쥐었다. 하지만 PPMI 행렬에도 여전히 큰 문제가 있다. 말뭉치의 어휘 수가 증가함에 따라 각 단어의 벡터의 차원 수도 증가한다는 문제이다. 예를 들어 말뭉치의 어휘 수가 10만 개라면 벡터의 차원 수도 10만이 된다. 10만 차원의 벡터를 다루는 것은 그다지 현실적이지 않다.\n",
        "\n",
        "또한, 이 행렬의 원소들 대부분이 0이다. 즉 벡터의 원소 대부분이 중요하지 않다는 뜻이다. 이런 벡터는 노이즈에 약하고 견고하지 못하다는 약점이 존재한다. 이를 대처하고자 자주 수행하는 기법이 바로 벡터의 차원 감소이다."
      ],
      "metadata": {
        "id": "uJxjOOP2rIFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **차원 감소**"
      ],
      "metadata": {
        "id": "htPIKroUr9os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**차원 감소(dimensionality reduction)**는 문자 그대로 벡터의 차원을 줄이는 방법을 말한다. 그러나 단순히 줄이기만 하는게 아니라, '중요한 정보'는 최대한 유지하면서 줄이는게 핵심이다. 데이터의 분포를 고려해 중요한 '축'을 찾는 일을 수행한다.\n",
        "\n",
        "원소 대부분이 0인 행렬 또는 벡터를 '희소행렬(sparse matrix)' 또는 '희소벡터(sparse vector)'라 한다. 차원 감소의 핵심은 희소벡터에서 중요한 축을 찾아내어 더 적은 차원으로 다시 표현하는 것인데, 차원 감소의 결과로 원래의 희소벡터는 원소 대부분이 0이 아닌 값으로 구성되는 '밀집벡터'로 변환된다. 이 조밀한 벡터야 말로 우리가 원하는 단어의 분산 표현이다.\n",
        "\n",
        "차원을 감소시키는 방법은 여러 가지가 있다. 그중에서 우리는 **특잇값분해(Singular Value Decomposition, SVD)**를 이용한다. SVD는 임의의 행렬을 세 행렬의 곱으로 분해하며, 수식으로는 다음과 같다.\n",
        "\n",
        "$$X=US{V}^T$$\n",
        "\n",
        "$U$와 $V$는 직교행렬(orthogonal matrix)이고, 그 열벡터는 서로 직교한다. 또한 $S$는 대각행렬(diagonal matrix)이다.\n",
        "\n",
        "$U$는 직교행렬이므로 어떠한 공간의 축(기저)을 형성한다. 우리의 맥락에서는 이 $U$ 행렬을 '단어 공간'으로 취급할 수 있다. 또한 $S$는 대각행렬로, 그 대각성분에는 '특잇값(singular value)'이 큰 순서로 나열되어 있다. 특잇값이란 쉽게 말해 '해당 축'의 중요도라고 간주할 수 있다. 따라서 중요도가 낮은 원소(특잇값이 작은 원소)를 깎아내는 방법을 생각할 수 있다.\n",
        "\n",
        "행렬 $S$에서 특잇값이 작다면 중요도가 낮다는 뜻이므로, 행렬 $U$에서 여분의 열벡터를 깎아내어 원래의 행렬을 근사할 수 있다. 이를 우리 문제로 가져와서 '단어의 PPMI 행렬'에 적용해보자."
      ],
      "metadata": {
        "id": "UUtWNt62r_dV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SVD에 의한 차원 감소**\n"
      ],
      "metadata": {
        "id": "RH7jsFxjwi-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(id_to_word)\n",
        "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
        "W = ppmi(C)\n",
        "\n",
        "# SVD\n",
        "U, S, V = np.linalg.svd(W)"
      ],
      "metadata": {
        "id": "mNsAgJwnwnEu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시\n",
        "print(C[0]) # 동시 발생 행렬의 벡터\n",
        "print(W[0]) # PPMI 행렬의 벡터\n",
        "print(U[0]) # SVD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO53Ve5Owznc",
        "outputId": "30fcfda9-5feb-48de-f456-49a4a8202b90"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 0 0 0]\n",
            "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            "[-1.110e-16  3.409e-01 -4.163e-16 -1.205e-01 -1.110e-16 -9.323e-01\n",
            " -1.086e-16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 결과에서 보듯 원래는 희소벡터인 W[0]이 SVD에 의해서 밀집벡터 U[0]으로 변했다. 또한 이 밀집벡터의 차원을 감소시키려면, 원하는 차원만큼의 원소만 꺼내면 된다."
      ],
      "metadata": {
        "id": "emeKYxH4w2cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(U[0, :2]) # 예로 2차원 벡터로 줄인다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_fNXfJWxN-4",
        "outputId": "15bbda14-4536-415a-f86c-7e451344e877"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.110e-16  3.409e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어를 2차원 벡터로 표현 후 시각화\n",
        "\n",
        "for word, word_id in word_to_id.items():\n",
        "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
        "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "0mLJ1v66xQmd",
        "outputId": "bff04dcd-fdf6-4126-cf49-d504cfd74d29"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "나름의 직관과 비교적 비슷하지만 지금 사용한 말뭉치가 아주 작아서 이 결과를 그대로 받아들이기 석연치 않다. 따라서 PTB 데이터셋이라는 더 큰 말뭉치를 사용하여 똑같은 실험을 수행해보자.  \n",
        "\n",
        "*행렬의 크기가 N이면 계산량이 N의 3제곱에 비례해 늘어나므로 현실적으로 감당하기 어려운 수준이다. 따라서 특잇값이 작은 것은 버리는(truncated)방식으로 성능 향상을 꾀하는 Truncated SVD 같은 더 빠른 기법을 이용한다.*"
      ],
      "metadata": {
        "id": "x2-NfsvExYiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PTB 데이터셋**"
      ],
      "metadata": {
        "id": "7N0YpjZ4x0yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PTB 데이터셋 불러오기\n",
        "\n",
        "from dataset import ptb\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "\n",
        "print('말뭉치 크기:', len(corpus))\n",
        "print('corpus[:30]:', corpus[:30])\n",
        "print()\n",
        "print('id_to_word[0]:', id_to_word[0])\n",
        "print('id_to_word[1]:', id_to_word[1])\n",
        "print('id_to_word[2]:', id_to_word[2])\n",
        "print()\n",
        "print(\"word_to_id['car']:\", word_to_id['car'])\n",
        "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
        "print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hmryDEzyNza",
        "outputId": "3ef1c394-d052-4d95-8bbf-1f6ee5635570"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "말뭉치 크기: 929589\n",
            "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29]\n",
            "\n",
            "id_to_word[0]: aer\n",
            "id_to_word[1]: banknote\n",
            "id_to_word[2]: berlitz\n",
            "\n",
            "word_to_id['car']: 3856\n",
            "word_to_id['happy']: 4428\n",
            "word_to_id['lexus']: 7426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PTB 데이터셋 평가**"
      ],
      "metadata": {
        "id": "Fw7s34shyTlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PTB 데이터셋에 통계 기법을 적용하는데, 큰 행렬에 SVD를 적용해야 하므로 사이킷런의 고속 SVD를 이용한다.\n",
        "\n",
        "그러면 시간도 절약하고 메모리도 훨씬 덜 사용할 수 있다."
      ],
      "metadata": {
        "id": "VggPlIjeycg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "wordvec_size = 100\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "print('동시발생 수 계산 ...')\n",
        "C = create_co_matrix(corpus, vocab_size, window_size)\n",
        "print('PPMI 계산 ...')\n",
        "W = ppmi(C, verbose=True)\n",
        "\n",
        "print('calculating SVD ...')\n",
        "try:\n",
        "    # truncated SVD (빠르다!)\n",
        "    from sklearn.utils.extmath import randomized_svd\n",
        "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
        "                             random_state=None)     # Truncated SVD는 무작위 수를 사용하므로 결과가 매번 다르다.\n",
        "except ImportError:\n",
        "    # SVD (느리다)\n",
        "    U, S, V = np.linalg.svd(W)\n",
        "\n",
        "word_vecs = U[:, :wordvec_size]\n",
        "\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OIlCh7Qykkz",
        "outputId": "2d180abe-6870-43a7-8ba3-b306c2a892c7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "동시발생 수 계산 ...\n",
            "PPMI 계산 ...\n",
            "1.0% 완료\n",
            "2.0% 완료\n",
            "3.0% 완료\n",
            "4.0% 완료\n",
            "5.0% 완료\n",
            "6.0% 완료\n",
            "7.0% 완료\n",
            "8.0% 완료\n",
            "9.0% 완료\n",
            "10.0% 완료\n",
            "11.0% 완료\n",
            "12.0% 완료\n",
            "13.0% 완료\n",
            "14.0% 완료\n",
            "15.0% 완료\n",
            "16.0% 완료\n",
            "17.0% 완료\n",
            "18.0% 완료\n",
            "19.0% 완료\n",
            "20.0% 완료\n",
            "21.0% 완료\n",
            "22.0% 완료\n",
            "23.0% 완료\n",
            "24.0% 완료\n",
            "25.0% 완료\n",
            "26.0% 완료\n",
            "27.0% 완료\n",
            "28.0% 완료\n",
            "29.0% 완료\n",
            "30.0% 완료\n",
            "31.0% 완료\n",
            "32.0% 완료\n",
            "33.0% 완료\n",
            "34.0% 완료\n",
            "35.0% 완료\n",
            "36.0% 완료\n",
            "37.0% 완료\n",
            "38.0% 완료\n",
            "39.0% 완료\n",
            "40.0% 완료\n",
            "41.0% 완료\n",
            "42.0% 완료\n",
            "43.0% 완료\n",
            "44.0% 완료\n",
            "45.0% 완료\n",
            "46.0% 완료\n",
            "47.0% 완료\n",
            "48.0% 완료\n",
            "49.0% 완료\n",
            "50.0% 완료\n",
            "51.0% 완료\n",
            "52.0% 완료\n",
            "53.0% 완료\n",
            "54.0% 완료\n",
            "55.0% 완료\n",
            "56.0% 완료\n",
            "57.0% 완료\n",
            "58.0% 완료\n",
            "59.0% 완료\n",
            "60.0% 완료\n",
            "61.0% 완료\n",
            "62.0% 완료\n",
            "63.0% 완료\n",
            "64.0% 완료\n",
            "65.0% 완료\n",
            "66.0% 완료\n",
            "67.0% 완료\n",
            "68.0% 완료\n",
            "69.0% 완료\n",
            "70.0% 완료\n",
            "71.0% 완료\n",
            "72.0% 완료\n",
            "73.0% 완료\n",
            "74.0% 완료\n",
            "75.0% 완료\n",
            "76.0% 완료\n",
            "77.0% 완료\n",
            "78.0% 완료\n",
            "79.0% 완료\n",
            "80.0% 완료\n",
            "81.0% 완료\n",
            "82.0% 완료\n",
            "83.0% 완료\n",
            "84.0% 완료\n",
            "85.0% 완료\n",
            "86.0% 완료\n",
            "87.0% 완료\n",
            "88.0% 완료\n",
            "89.0% 완료\n",
            "90.0% 완료\n",
            "91.0% 완료\n",
            "92.0% 완료\n",
            "93.0% 완료\n",
            "94.0% 완료\n",
            "95.0% 완료\n",
            "96.0% 완료\n",
            "97.0% 완료\n",
            "98.0% 완료\n",
            "99.0% 완료\n",
            "calculating SVD ...\n",
            "\n",
            "[query] you\n",
            " i: 0.6973986625671387\n",
            " do: 0.5475264191627502\n",
            " else: 0.5083335041999817\n",
            " we: 0.5082763433456421\n",
            " anybody: 0.5024328231811523\n",
            "\n",
            "[query] year\n",
            " month: 0.6721916794776917\n",
            " earlier: 0.6493456959724426\n",
            " june: 0.6341113448143005\n",
            " quarter: 0.6283566951751709\n",
            " next: 0.6260337233543396\n",
            "\n",
            "[query] car\n",
            " luxury: 0.6482807397842407\n",
            " auto: 0.6382479071617126\n",
            " truck: 0.5611585378646851\n",
            " domestic: 0.530119776725769\n",
            " lexus: 0.5118359923362732\n",
            "\n",
            "[query] toyota\n",
            " motor: 0.7299633026123047\n",
            " motors: 0.6784077286720276\n",
            " lexus: 0.654114305973053\n",
            " mazda: 0.6380295157432556\n",
            " nissan: 0.6048797965049744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과를 보면 우리의 직관과 비슷한 결과를 확인할 수 있다. 대규모 말뭉치를 사용하면 단어의 분산 표현의 품질이 더 좋아지는 것을 알 수 있다."
      ],
      "metadata": {
        "id": "aJJKRF--yx8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **정리**"
      ],
      "metadata": {
        "id": "ofZnpwtqy-AT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **WordNet 등의 시소러스를 이용하면 유의어를 얻거나 단어 사이의 유사도를 측정하는 등 유용한 작업을 할 수 있다.**\n",
        "* **시소러스 기반 기법은 시소러스를 작성하는 데 엄청난 인적 자원이 든다거나 새로운 단어에 대응하기 어렵다는 문제가 있다.**\n",
        "* **현재는 말뭉치를 이용해 단어를 벡터화하는 방식이 주로 쓰인다.**\n",
        "* **최근의 단어 벡터화 기법들은 대부분 '단어의 의미는 주변 단어에 의해 형성된다'는 분포 가설에 기초한다.**\n",
        "* **통계 기반 기법은 말뭉치 안의 각 단어에 대해서 그 단어의 주변 단어의 빈도를 집계한다(동시발생 행렬).**\n",
        "* **동시발생 행렬을 PPMI 행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한 '희소벡터'를 작은 '밀집벡터'로 변환할 수 있다.**\n",
        "* **단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것으로 기대된다.**"
      ],
      "metadata": {
        "id": "DYSXKkZ-zuXk"
      }
    }
  ]
}