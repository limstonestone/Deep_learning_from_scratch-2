{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap04_word2vec_speed_improvement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRUB2tELbP00Jmg5UfD7th"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlQYbFe23L-q",
        "outputId": "6803e24a-1509-4f88-d6ff-ef7cc2abe258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **word2vec 개선-1**"
      ],
      "metadata": {
        "id": "1hW6v-LC3iW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞 장의 CBOW 모델에서 어휘가 100만 개, 은닉층의 뉴런이 100개인 CBOW 모델을 생각해보면, 입력층과 출력층에는 각 100만개의 뉴런이 존재한다. 이 수많은 뉴런 때문에 중간 계산에 많은 시간이 소요된다. 정확히는 다음의 두 계산이 병목이 된다.\n",
        "\n",
        "* 입력층의 원핫 표현과 가중치 행렬 $W_{in}$의 곱 계산\n",
        "* 은닉층과 가중치 행렬 $W_{out}$의 곱 및 Softmax 계층의 계산\n",
        "\n",
        "첫 번째는 입력층의 원핫 표현과 관련한 문제이다. 단어를 원핫 표현으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커진다. 상당한 메모리를 차지하게 된다는 말이다. 이 문제는 Embedding 계층을 도입하는 것으로 해결한다.\n",
        "\n",
        "두 번째는 은닉층 이후의 계산 문제이다. 우선 은닉층과 가중치 행렬 $W_{out}$의 곱만 해도 계산량이 상당하다. 그리고 Softmax 계층에서도 다루는 어휘가 많아짐에 따라 계산량이 증가하는 문제가 있다. 이 문제는 네거티브 샘플링이라는 새로운 손실 함수를 도입해 해결한다."
      ],
      "metadata": {
        "id": "OUFX2qUp3ubp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedding 계층**"
      ],
      "metadata": {
        "id": "EeLKqDtg4p-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "원핫 표현에서 결과적으로 수행하는 일은 단지 행렬의 특정 행을 추출하는 것 뿐인데, 앞에서의 Matmul 계층의 행렬 곱은 거대한 벡터와 가중치 행렬을 곱한다. 원핫 표현으로의 변환과 MatMul 계층의 행렬 곱 계산은 사실 필요가 없는 것이다.\n",
        "\n",
        "그러면 가중치 매개변수로부터 '단어 ID에 해당하는 행(벡터)'을 추출하는 계층을 만들어보자. 그 계층을 Embedding 계층이라고 부른다. Embedding이란, Embedding 계층에 단어 임베딩(분산 표현)을 저장하는 것이다.\n",
        "\n",
        "참고로, 통계 기반 기법으로 얻은 단어 벡터는 영어로 distributional representation이라고 하고, 신경망을 사용한 추론 기반 기법으로 얻은 단어 벡터는 distributed representation이라고 한다. 이 책에서는 둘 다 '분산 표현'으로 번역한다."
      ],
      "metadata": {
        "id": "JeOdiMBx4s3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedding 계층 구현**"
      ],
      "metadata": {
        "id": "NbGQ5OjR5S__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "W = np.arange(21).reshape(7, 3)\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHkPtd-x5W96",
        "outputId": "953b97d7-7c21-4340-9af7-8804e1412758"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2],\n",
              "       [ 3,  4,  5],\n",
              "       [ 6,  7,  8],\n",
              "       [ 9, 10, 11],\n",
              "       [12, 13, 14],\n",
              "       [15, 16, 17],\n",
              "       [18, 19, 20]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W[2] # 단지 원하는 행을 명시하기만 하면 끝"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsAMSUp85bdq",
        "outputId": "8aea46a6-2149-4a07-a42f-158de1cfaa6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 7, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.array([1, 0, 3, 0]) # 원하는 행 번호들을 명시하기만 하면 끝\n",
        "W[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD553IUy5csn",
        "outputId": "ab31e50e-0c9d-404a-bbf7-bf12806d1636"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  4,  5],\n",
              "       [ 0,  1,  2],\n",
              "       [ 9, 10, 11],\n",
              "       [ 0,  1,  2]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding 계층의 forward() 메서드를 구현해보자."
      ],
      "metadata": {
        "id": "1YWfQwN25nWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "\n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "\n",
        "    # Embedding 계층의 backward() 메서드를 구현해보자. 순전파와 마찬가지로 원하는 행에만 기울기를 전달한다.\n",
        "    def backward(self, dout):\n",
        "        dW, = self.grads\n",
        "        dW[...] = 0\n",
        "\n",
        "        for i, word_id in enumerate(self.idx):\n",
        "            dW[word_id] += dout[i]      # 인덱스가 중복되었을 경우 먼저 쓰여진 값을 덮어쓰는 문제를 방지하기 위해 '할당'이 아닌 '더하기'를 한다.\n",
        "        # 혹은\n",
        "        # np.add.at(dW, self.idx, dout)\n",
        "        # 넘파이 내장 메서드를 사용하면 효율이 훨씬 좋아진다.\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "bpE5PFaJ5raS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이상으로 word2vec (CBOW 모델)의 구현은 입력 측 MatMul 계층을 Embedding 계층으로 전환하여 메모리 사용량을 줄이고 쓸데없는 계산도 생략할 수 있게 되었다."
      ],
      "metadata": {
        "id": "iVkNuMH06Aay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **word2vec 개선-2**"
      ],
      "metadata": {
        "id": "DF4PPAen7bp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "은닉층 이후의 처리(행렬 곱과 Softmax 계층의 계산)문제를 해소 하기 위하여 **네거티브 샘플링(부정적 샘플링)**이라는 기법을 사용한다.\n",
        "\n",
        "Softmax 대신 네거티브 샘플링을 이용하면 어휘가 아무리 많아져도 계산량을 낮은 수준에서 일정하게 억제할 수 있다."
      ],
      "metadata": {
        "id": "CbxYEZnR7d3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **은닉층 이후 계산의 문제점**"
      ],
      "metadata": {
        "id": "ZCs5EdAc8App"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding 계층을 도입하여 입력층 계산에서의 낭비를 줄였지만, 은닉층 이후의 처리에서 문제가 있다. 은닉층 이후에서 계산이 오래 걸리는 곳은 다음의 두 부분이다.\n",
        "\n",
        "* 은닉층의 뉴런과 가중치 행렬($W_{out}$)의 곱\n",
        "* Softmax 계층의 계산\n",
        "\n",
        "첫 번째는 거대한 행렬을 곱하는 문제이다. 큰 행렬을 계산하려면 시간이 오래걸리고 메모리도 많이 필요하고, 역전파 때도 같은 계산을 수행하기 때문에 이 행렬 곱을 '가볍게' 만들어야 한다.\n",
        "\n",
        "두 번째로, Softmax에서도 같은 문제가 발생한다. 어휘가 100만개 일때의 Softmax 수식을 살펴보자.\n",
        "\n",
        "$$ y_k = \\frac{exp(s_k)}{\\sum_{i=1}^{1000000} exp(s_i)} $$\n",
        "\n",
        "이와 같이 분모의 값을 얻으려면 exp 계산을 100만 번 수행해야 한다. 이 계산도 어휘 수에 비례해 증가하므로 Softmax를 대신할 '가벼운' 계산이 절실하다."
      ],
      "metadata": {
        "id": "gsB1Wfh68DIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **다중 분류에서 이진 분류로**"
      ],
      "metadata": {
        "id": "Qh40CnXw-v4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "네거티브 샘플링 기법이란, 간단히 말해서 '다중 분류(multi-classification)'를 '이진 분류(binary-classification)'로 근사하는 것이다.\n",
        "\n",
        "예를 들어, \"맥락이 'you'와 'goodbye'일 때, 타깃 단어는 무엇입니까?\"라는 질문이 아니라, \"맥락이 'you'와 'goodbye'일 때, 타깃 단어는 'say' 입니까?\" 라는 질문에 답하는 신경망을 생각해내는 것이다.\n",
        "\n",
        "이렇게 하려면 출력층에는 뉴런을 하나만 준비하면 된다. 즉 출력층의 뉴런이 \"say\"의 점수를 출력한다."
      ],
      "metadata": {
        "id": "U6VGaKaK-xw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **시그모이드 함수와 교차 엔트로피 오차**"
      ],
      "metadata": {
        "id": "Y2oRTMNx_lz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이진 분류 문제를 신경망으로 풀려면 점수에 시그모이드 함수를 적용해 확률로 변환하고, 손실을 구할 때는 손실 함수로 '교차 엔트로피 오차'를 사용한다. 이 둘은 이진 분류 신경망에서 가장 흔하게 사용하는 조합이다. (다중 분류의 경우, 출력층에서 시그모이드 함수가 아닌 '소프트 맥스 함수'를 사용한다.)\n",
        "\n",
        "시그모이드 함수의 출력($y$)는 소프트맥스 함수의 출력과 마찬가지로 '확률'로 해석할 수 있다. 시그모이드 함수에 사용되는 손실함수 '교차 엔트로피 오차'는 다음과 같이 쓸 수 있다.\n",
        "\n",
        "$$ L = -(tlogy + (1-t)log(1-y)) $$\n",
        "\n",
        "$t$가 1이면 $-logy$가 출력되고, 반대로 $t$가 0이면 $-log(1-y)$가 출력된다.\n",
        "또한 소프트 맥스와 마찬가지로 역전파의 값은 $y-t$, 즉 정답과 출력의 오차이다."
      ],
      "metadata": {
        "id": "tKIbfAxw_orL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **다중 분류에서 이진 분류로 (구현)**"
      ],
      "metadata": {
        "id": "B6n2wjps_1gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선, 앞서 설명한 Embedding 계층과 'dot 연산(내적)'의 처리를 합친 계층인 Embedding Dot 계층을 도입하자. 그 구현은 다음과 같다."
      ],
      "metadata": {
        "id": "ymmL5O49Bevk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingDot:\n",
        "    def __init__(self, W):\n",
        "        self.embed = Embedding(W)\n",
        "        self.params = self.embed.params\n",
        "        self.grads = self.embed.grads\n",
        "        self.cache = None       # 순전파 시의 계산 결과를 잠시 유지하기 위한 변수로 사용\n",
        "\n",
        "    def forward(self, h, idx):\n",
        "        target_W = self.embed.forward(idx)\n",
        "        out = np.sum(target_W * h, axis=1)\n",
        "\n",
        "        self.cache = (h, target_W)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        h, target_W = self.cache\n",
        "        dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "        dtarget_W = dout * h\n",
        "        self.embed.backward(dtarget_W)\n",
        "        dh = dout * target_W\n",
        "        return dh"
      ],
      "metadata": {
        "id": "Se7EM5LTCcre"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **네거티브 샘플링**"
      ],
      "metadata": {
        "id": "PR6LD9aoDNlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 배운 것으로 주어진 문제를 '다중 분류'에서 '이진 분류'로 변환할 수 있었지만, 긍정적인 예(정답)에 대해서만 학습했기 때문에 부정적인 예(오답)를 입력하면 어떤 결과가 나올지 확실하지 않다.\n",
        "\n",
        "우리가 하고싶은 일은 긍정적 예(\"say\")에 대해서는 Sigmoid 계층의 출력을 1에 가깝게 만들고, 부정적 예(\"say\"이외의 단어)에 대해서는 Sigmoid 계층의 출력을 0에 가깝게 만드는 것이다. 그리고 이런 결과를 만들어주는 가중치가 필요하다.\n",
        "\n",
        "모든 부정적 예를 대상으로 하여 이진 분류를 학습시키면, 어휘 수 증가에 대처하는 것이 목적인 것에 반한다. 그래서 근사적인 해법으로, 부정적 예를 몇 개(선택하는 방법은 뒤에서 다룬다) 선택한다. 즉, 적은 수의 부정적 예를 샘플링 해 사용한다.\n",
        "\n",
        "이것이 바로 '네거티브 샘플링' 기법이 의미하는 바이다.\n",
        "\n",
        "정리하면, 네거티브 샘플링 기법은 긍정적 예를 타깃으로 한 경우의 손실을 구하고, 동시에 부정적 예를 몇 개 샘플링하여, 그 부정적 예에 대해서도 마찬가지로 손실을 구한다. 그리고 각자의 데이터의 손실을 더한 값을 최종 손실로 한다."
      ],
      "metadata": {
        "id": "kpfXsUN5DQSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **네거티브 샘플링의 샘플링 기법**"
      ],
      "metadata": {
        "id": "bftebR3-D-SH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "부정적 예를 어떻게 샘플링 하냐는 질문에 답하자면, 말뭉치의 통계 데이터를 기초로 샘플링하면 된다. 말뭉치에서 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출하는 것이다.\n",
        "\n",
        "말뭉치에서의 단어별 출현 횟수를 바탕으로 확률분포를 구한 다음, 그 확률분포에 따라서 샘플링을 수행하기만 하면 된다. 따라서 '희소한 단어'는 선택되기가 어렵다.\n",
        "\n",
        "우연히 '희소한 단어'만 선택하면 결과도 나빠질 것이다. 실전 문제에서도 희소한 단어는 거의 출현하지 않기 때문에, 드문 단어를 잘 처리하는 일은 중요도가 낮다. 그보다 흔한 단어를 잘 처리하는 편이 좋은 결과를 낳는다.\n",
        "\n",
        "확률분포에 따라 샘플링하는 예를 파이썬 코드로 확인해보자."
      ],
      "metadata": {
        "id": "5yt7xOabFniW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0에서 9까지의 숫자 중 하나를 무작위로 샘플링\n",
        "np.random.choice(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx71xehhGdJk",
        "outputId": "d91e1f55-38a7-469a-b83f-d921c699550f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1GOjAn1GnEB",
        "outputId": "79ad0a0d-f1fb-43c2-a330-c4edb1e8ba01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# words에서 하나만 무작위로 샘플링\n",
        "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
        "np.random.choice(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eNqZdGRFGosZ",
        "outputId": "91d8a003-d37b-4193-9109-467f7395ae5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5개만 무작위로 샘플링(중복 있음)\n",
        "np.random.choice(words, size=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVsLzu9IGyGB",
        "outputId": "68daf48b-80ff-4216-f529-5ac35cefde36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['say', 'you', 'hello', 'hello', '.'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5개만 무작위로 샘플링(중복 없음)\n",
        "np.random.choice(words, size=5, replace=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJaSFj3ZG2ap",
        "outputId": "b4cf744c-07e6-45b5-e578-e05d0ce52f62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['I', 'hello', 'you', '.', 'goodbye'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 확률분포에 따라 샘플링\n",
        "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
        "np.random.choice(words, p=p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "U2aBiWRoG7_c",
        "outputId": "dee369af-cae6-4576-da01-56fa8f547746"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'say'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec의 네거티브 샘플링에서는 기본 확률분포에 0.75를 제곱한다.\n",
        "\n",
        "$$P^{'}(w_i)=\\frac{P(w_i)^{0.75}}{\\sum_{j}^n P(w_j)^{0.75}}$$\n",
        "\n",
        "그 이유는 출현 확률이 낮은 단어를 '버리지 않기' 위해서이다. 정확히 말해서, '0.75 제곱'을 함으로써, 원래 확률이 낮은 단어의 확률을 살짝 높일 수 있다. 예를 보자."
      ],
      "metadata": {
        "id": "o-c4BZKpHBVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = [0.7, 0.29, 0.01]\n",
        "new_p = np.power(p, 0.75)\n",
        "new_p /= np.sum(new_p)\n",
        "print(new_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc1xo79hHZ-Q",
        "outputId": "54f87100-42b7-47a5-d4e8-1c8d2ab672f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.64196878 0.33150408 0.02652714]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전엔 확률이 0.01이던 원소가 수정 후에는 0.0265.. 가 되었다. 낮은 확률의 단어가 (조금 더)쉽게 샘플링되도록 하기 위한 구제 조치로써 '0.75제곱'을 수행한다. 0.75라는 수치에는 이론적인 의미는 없으니 다른 숫자로 설정해도 된다.\n",
        "\n",
        "이 처리를 담당하는 UnigramSampler 클래스를 구현하는데, 자세한 설명은 하지 않겠다."
      ],
      "metadata": {
        "id": "8Hj3Fvj7HgCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        if not GPU:\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()\n",
        "                target_idx = target[i]\n",
        "                p[target_idx] = 0\n",
        "                p /= p.sum()\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
        "        else:\n",
        "            # GPU(cupy）로 계산할 때는 속도를 우선한다.\n",
        "            # 부정적 예에 타깃이 포함될 수 있다.\n",
        "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
        "                                               replace=True, p=self.word_p)\n",
        "\n",
        "        return negative_sample"
      ],
      "metadata": {
        "id": "grIFn9JXIpIJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **네거티브 샘플링 구현**"
      ],
      "metadata": {
        "id": "0ontp9maH-oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSamplingLoss:\n",
        "\n",
        "    # 초기화 메서드\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    # 순전파 구현\n",
        "    def forward(self, h, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "        # 긍정적 예 순전파\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        # 부정적 예 순전파\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]\n",
        "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # 역전파 구현\n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "\n",
        "        return dh"
      ],
      "metadata": {
        "id": "uJrXabzRIMCl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **개선판 word2vec 학습**"
      ],
      "metadata": {
        "id": "9qH48JhFI9i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CBOW 모델 구현**"
      ],
      "metadata": {
        "id": "HHQ9mWNgI1nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/밑바닥부터시작하는딥러닝2/deep-learning-from-scratch-2-master'\n",
        "import sys\n",
        "sys.path.append(path)\n",
        "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
        "\n",
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = Embedding(W_in)  # Embedding 계층 사용\n",
        "            self.in_layers.append(layer)\n",
        "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        # 모든 가중치와 기울기를 배열에 모은다.\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])\n",
        "        h *= 1 / len(self.in_layers)\n",
        "        loss = self.ns_loss.forward(h, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.ns_loss.backward(dout)\n",
        "        dout *= 1 / len(self.in_layers)\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout)\n",
        "        return None"
      ],
      "metadata": {
        "id": "0VMN_rq7I5m-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CBOW 모델 학습 코드**"
      ],
      "metadata": {
        "id": "u0wIS7ZkJGxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from common import config\n",
        "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
        "# ===============================================\n",
        "config.GPU = True\n",
        "# ===============================================\n",
        "import pickle\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from ch04.skip_gram import SkipGram\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu\n",
        "from dataset import ptb\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "# 데이터 읽기\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "\n",
        "if config.GPU:\n",
        "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
        "\n",
        "# 모델 등 생성\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "# 학습 시작\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "\n",
        "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
        "word_vecs = model.word_vecs\n",
        "if config.GPU:\n",
        "    word_vecs = to_cpu(word_vecs)\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "    pickle.dump(params, f, -1)"
      ],
      "metadata": {
        "id": "xDSC0r4kJVmM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "버전 호환때문에 오류가 뜨므로 pickle을 이용해보자."
      ],
      "metadata": {
        "id": "5Ks1W_jFJpN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CBOW 모델 평가**"
      ],
      "metadata": {
        "id": "8JVXaFtySfvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from common.util import most_similar, analogy\n",
        "import pickle\n",
        "\n",
        "pkl_file = path + '/ch04/cbow_params.pkl'\n",
        "# pkl_file = 'skipgram_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "    word_vecs = params['word_vecs']\n",
        "    word_to_id = params['word_to_id']\n",
        "    id_to_word = params['id_to_word']\n",
        "\n",
        "# 가장 비슷한(most similar) 단어 뽑기\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n48YB3VeSie1",
        "outputId": "526edb9f-7364-4b1a-86b1-0aa254a3d45a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.6103515625\n",
            " someone: 0.59130859375\n",
            " i: 0.55419921875\n",
            " something: 0.48974609375\n",
            " anyone: 0.47314453125\n",
            "\n",
            "[query] year\n",
            " month: 0.71875\n",
            " week: 0.65234375\n",
            " spring: 0.62744140625\n",
            " summer: 0.6259765625\n",
            " decade: 0.603515625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.497314453125\n",
            " arabia: 0.47802734375\n",
            " auto: 0.47119140625\n",
            " disk-drive: 0.450927734375\n",
            " travel: 0.4091796875\n",
            "\n",
            "[query] toyota\n",
            " ford: 0.55078125\n",
            " instrumentation: 0.509765625\n",
            " mazda: 0.49365234375\n",
            " bethlehem: 0.47509765625\n",
            " nissan: 0.474853515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과를 보면 CBOW 모델로 획득된 단어의 분산 표현은 제법 괜찮은 특성을 지닌다고 말할 수 있다.\n",
        "\n",
        "word2vec으로 얻은 단어의 분산 표현은 비슷한 단어를 가까이 모을 뿐 아니라, 더 복잡한 패턴을 파악하는 것으로 알려져 있다. 예를 들어 \"king - man + woman = queen\"으로 유명한 유추 문제이다. 정확하게 말하면, word2vec의 단어의 분산 표현을 사용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있다는 뜻이다."
      ],
      "metadata": {
        "id": "_IkE_9haStDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 유추(analogy) 작업\n",
        "print('-'*50)\n",
        "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuy17W0dTVlL",
        "outputId": "abc493ab-967f-4a28-d62b-748def4e46c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "\n",
            "[analogy] king:man = queen:?\n",
            " woman: 5.16015625\n",
            " veto: 4.9296875\n",
            " ounce: 4.69140625\n",
            " earthquake: 4.6328125\n",
            " successor: 4.609375\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 4.55078125\n",
            " points: 4.25\n",
            " began: 4.09375\n",
            " comes: 3.98046875\n",
            " oct.: 3.90625\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " children: 5.21875\n",
            " average: 4.7265625\n",
            " yield: 4.20703125\n",
            " cattle: 4.1875\n",
            " priced: 4.1796875\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " more: 6.6484375\n",
            " less: 6.0625\n",
            " rather: 5.21875\n",
            " slower: 4.734375\n",
            " greater: 4.671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단수형과 복수형을 올바르게 파악하고, 비교급 단어등을 제시하는 것을 보면 단어의 단순한 의미뿐 아니라 문법적인 패턴도 파악할 수 있음을 알 수 있다."
      ],
      "metadata": {
        "id": "E5iQxMxsTWyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **word2vec을 사용한 애플리케이션의 예**"
      ],
      "metadata": {
        "id": "3y-dxLN-Tfhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec으로 얻은 단어의 분산 표현은 비슷한 단어를 찾는 용도로 이용할 수 있다. 그러나 장점은 여기서 끝나는 것이 아니다. 자연어 처리 분야에서 단어의 분산 표현이 중요한 이유는 **전이 학습(transfer learning)**에 있다. 전이 학습은 한 분야에서 배운 지식을 다른 분야에도 적용하는 기법이다.\n",
        "\n",
        "자연어 문제를 풀 때 word2vec의 단어 분산 표현을 처음부터 학습하는 일은 거의 없다. 먼저 큰 말뭉치(위키백과 등)으로 학습을 끝난 후, 그 분산 표현을 각자의 작업에 이용한다.\n",
        "\n",
        "단어의 분산 표현은 단어를 고정 길이 벡터로 변환해준다는 장점도 있다. 문장 또한 단어의 분산 표현을 사용하여 고정 길이 벡터로 변환할 수 있다. 이 중 가장 간단한 방법은 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구하는 bag-of-words 가 있다. 이 모델은 순서를 고려하지 않는다.\n",
        "\n",
        "단어나 문장을 고정 길이로 변환해주는 것이 장점이 되는 이유는 자연어를 벡터로 변환할 수 있다면 일반적인 머신러닝 기법(신경망이나 SVM 등)을 적용할 수 있기 때문이다.\n",
        "\n",
        "이를 통해 감성분석 등의 애플리케이션이 가능하다."
      ],
      "metadata": {
        "id": "ToOolmT4TjU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **단어 벡터 평가 방법**"
      ],
      "metadata": {
        "id": "KPLWdjS7UPFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어의 분산 표현이 어떻게 평가할까에 대해서 논해보자.\n",
        "\n",
        "단어의 분산 표현은 애플리케이션에서 사용되는 것이 대부분이므로, 궁극적으로 정확도 높은 시스템을 구축해야한다. 그 시스템은 여러 시스템으로 구성되는 것을 생각해보아야 한다. 단어의 분산 표현을 만드는 시스템(word2vec)과 특정 문제에 대해 분류를 수행하는 시스템(감정을 분류하는 SVM 등)이 그 예이다.\n",
        "\n",
        "단어의 분산 표현의 우수성은 실제 애플리케이션과는 분리해 평가하는 것이 일반적이다(함께 하면 시간이 오래 걸릴 수 있다). 이때 자주 사용되는 평가 척도가 단어의 '유사성'이나 '유추 문제'를 활용한 평가이다.\n",
        "\n",
        "단어의 유사성 평가에서는 사람이 작성한 단어 유사도를 검증 세트로 사용해 평가하는 것이 일반적이다. 사람이 부여한 점수와 word2vec에 의한 코사인 유사도 점수를 비교해 그 상관성을 보는것이다.\n",
        "\n",
        "유추 문제를 활용한 평가에서는 유추 문제를 출제하고, 그 정답률로 단어의 분산 표현의 우수성을 측정한다. 유추 문제를 이용하면 '단어의 의미나 문법적인 문제를 제대로 이해하고 있는지'를 어느 정도 측정할 수 있다.\n",
        "\n",
        "하지만 유추 문제에 의한 평가가 높다고 해서 애플리케이션에서도 반드시 좋은 결과가 나오리라는 보장은 없음을 주의해야 한다."
      ],
      "metadata": {
        "id": "8mkFecaJUccW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **정리**"
      ],
      "metadata": {
        "id": "umOPNJhjWhXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출한다.**\n",
        "* **word2vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로, 근사치로 계산하는 빠른 기법을 사용하면 좋다.**\n",
        "* **네거티브 샘플링은 부정적 예를 몇 개 샘플링하는 기법으로, 이를 이용하면 다중 분류를 이진 분류처럼 취급할 수 있다.**\n",
        "* **word2vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치한다.**\n",
        "* **word2vec의 단어의 분산 표현을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있게 된다.**\n",
        "* **word2vec은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업에 이용할 수 있다.**"
      ],
      "metadata": {
        "id": "3r9JCbEsWiUB"
      }
    }
  ]
}